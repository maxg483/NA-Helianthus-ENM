# Occurrence_Data_Cleaning.R
## Occurrence Data Cleaning
## Modified based on script written by Charlotte Germain-Aubrey.
## Modified and created by ML Gaynor
## Further edited by Max Gebhart

### Load Packages
library(dplyr)
library(tidyr)
library(rjson)  
library(RCurl) 
library(raster)
library(sp)
install.packages("CoordinateCleaner")
library(CoordinateCleaner)
install.packages("spThin")
library(spThin)

### Load data files
## While editing the code it is better to use the TESTER dataset in order
## to prevent any unwanted changes within the master data
Tester.raw.data <- read.csv("NA_Data/Master_Hel_DataTESTER.csv")
raw.data <- read.csv("NA_Data/Master_Hel_Data52820.csv")
head(raw.data)

#### Exploring the datafile 
##### How many species are included in this file?
(species_count <- raw.data %>%
    group_by(name) %>%
    tally())

write.csv(species_count, "NA_Data/Species_Count52820.csv")


# Taxonomic Cleaning
## This can change misspellings, synonyms, and more with messy data 
## using the **iPlant TNRS Tool**
## This step was omitted while working with Helianthus since names were
## changed during the data download step

{
## In addition to accessing the iPlant TNRS Tool in R, you can also use the 
## website to access the database (http://tnrs.iplantcollaborative.org/). 
## In this tutorial, we only focus on checking synoymns using one tool, 
## however there are many other tools to check taxonomy.
## For example, taxize(https://ropensci.github.io/taxize-book/index.html) can 
## be used to accesss taxomny databases including Encylopedia of Life (eol), 
## Taxonomic Name Resolution Service (tnrs), Integrated Taxonomic Information Service (itis), 
## and [many more](https://ropensci.github.io/taxize-book/data-sources.html). 

#### Using the iPlant TNRS tool

##### Transform the species names list
###### First, we need to subset the names from the list we made above.

# Subset from the list we made above
(names <- species_count$species)

# Next, remove the underscores between the genus and species name, replace with spaces.
(names<-gsub('_',' ',names))

# Turn the list into a string by adding commas.
(names<-paste(names,collapse=','))

# Using the package **RCurl** makes the string URL-encoded.
names<-curlEscape(names)

##### Query the API
# Using the packages **rjson** and **RCurl** we can query the Application programming interface (API) for the iPlant TNRS tool.  
# We have to set an object equal to the API url address.
tnrs.api<-'http://tnrs.iplantc.org/tnrsm-svc'

# Next, we send a request to the address specified above. 
url<-paste(tnrs.api,'/matchNames?retrieve=best&names=',names,sep='')
tnrs.json<-getURL(url) 

##### Process the results
# The response is saved as tnrs.json above. Now, the response needs to be converted to a readable format from JSON. 
tnrs.results<-fromJSON(tnrs.json)

# Next, we need to summarize the names and change the format to match our raw data.
# First we isolate the names submitted and the found accepted name.
# Next, we replace the space in between the genus and species names with an underscore. 
# Then, we convert the data file to a dataframe. Finally, we rename the columns to match the names in raw.data.

corrected_names<-sapply(tnrs.results[[1]], function(x) c(x$nameSubmitted,x$acceptedName))
corrected_names <- gsub(" ", "_", corrected_names)
corrected_names<-as.data.frame(t(corrected_names),stringsAsFactors=FALSE)
names(corrected_names) <- c("species", "new")
corrected_names

##### Correcting names
# We now need to correct the names in the raw data file. To do this, we are going to merge the datasets, 
# so we can save the new name with all the information in our raw data file.
merged_datasets <- merge(raw.data, corrected_names, by = "species")
head(merged_datasets)
}

### Date Cleaning
# In addition to taxonmy fixes, we need to check the format of other columns, like dates.
# If you noticed above, when the year is missing there is an odd symbol. Lets replace that symbol with "NA"
raw.data$date <- gsub("\\N", NA, raw.data$date)

## Is there any other issues? 
# Visualize the year and data associated. 

(year_count <- raw.data %>%
    group_by(date) %>%
    tally())

write.csv(year_count, "NA_Data/Year_Count52820.csv")

### Location Cleaning
#### Precision
# How precise is the locality information? 

# Precision is influenced by the number of decimal places associated with latitude and longitude. 
# See below how degree and distance are related. 

location_table <- data.frame(places = c(0,1,2,3,4), 
                             degree = c(1, 0.1, 0.01, 0.001, 0.0001), 
                             distance = c("111 km", "11.1 km", "1.11 km", "111 m", "11.1 m"))

location_table 

# Here we are going to round to two decimal points. 

{(raw.data$latitude <- round(raw.data$latitude, digits = 2))
raw.data$longitude <- round(raw.data$longitude, digits = 2)}

#### Removing impossible points

new.data <- raw.data %>%
  filter(latitude != 0, longitude != 0)
 
# Remove points that are botanical gardens and other institutions 
# that could skew data 
## The default is 100m away 

newer.data <- cc_inst(new.data, lon = "longitude", lat = "latitude", species = "name")

#### Removing duplicates

newest_data <- newer.data %>%
  distinct
head(newest_data) 

#### Trimming to the desired area 
# Our desired area is North America
# Using the package **raster**  we are able to create a raster layer of North America
#If you want to download other countries using getData, check out the [avaliable maps](https://gadm.org/maps.html).

# name = 'GADM' which is Database of Global Administrative Areas
# level = 2 gives the  level of subdivision (States)
Canada <- getData('GADM', country='CAN', level=2)
U.S.A <- getData('GADM', country='USA', level=2)
Mexico <- getData('GADM', country='MEX', level=2)

# Next, convert merged dataset unique into a spatial file. 

xy_data <- data.frame(x = newest_data$longitude, y = newest_data$latitude)
coordinates(xy_data) <- ~ x + y
proj4string(xy_data) <- crs("+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0")

# Extract the values of the points for the North America polygons.
# Then, recombine everything to gather all data. 
# Next, filter for only the points in North America, select the columns needed for MaxEnt input, and rename the new names as simply 'species'.
overU <- over(xy_data, U.S.A)
overC <- over(xy_data, Canada)
overM <- over(xy_data, Mexico)

totalCA <- cbind(newest_data, overC)
totalUS <- cbind(newest_data, overU)
totalME <- cbind(newest_data, overM)

Canada_points <- totalCA %>%
  filter(GID_0 == "CAN") %>%
  dplyr::select(name, longitude, latitude) 

USA_points <- totalUS %>%
  filter(GID_0 == "USA") %>%
  dplyr::select(name, longitude, latitude)

Mexico_points <- totalME %>%
  filter(GID_0 == "MEX") %>%
  dplyr::select(name, longitude, latitude)

#rename(replace = c("species" = "new")) # AEM EDIT


### Finally, write a csv for the maxent steps.

# We need a csv with only species, lat, long for MaxEnt input
write.csv(rbind.data.frame(Canada_points, USA_points, Mexico_points), "NA_Data/NAMaxEntcleaned.csv", row.names = FALSE)

